{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** = A neuron computes the weighted average of its inputs then passes the sum to an activation functino which determines how much of the signal is then passed on to the next neuron/layer.\n",
    "- **Input Layer:** = The provider of initial, unprocessed information to the neaural network/hidden layers.\n",
    "- **Hidden Layer:** = Isolated within the network with no contact to outside world, the bulk of the processing of data is usually done within the hidden layer. This layer eventually then transmits the results to the output layer.\n",
    "- **Output Layer:** = Responsible for computation and eventual transmission/transferrance to the outside world.\n",
    "- **Activation:** = (Function?) The function which takes in the weighted sums of inputs plus some bias term from the previous layer and outputs an activation value.\n",
    "- **Backpropagation:** = A recursive process during model training which starts with randomly assigned weights for inputs/wires-between-layers and, through a repeating process of observing the output of the weights/neurons and comparing to known values, updates the weights of the previous layers to bring the predictions/observations closer to the known values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')\n",
    "# candy_np = pd.read_csv('chocolate_gummy_bears.csv', header=None, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "candy['bias'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy','bias']].values\n",
    "y = candy['ate'].values\n",
    "correct_outputs = y.tolist()\n",
    "correct_outputs_np = np.asarray(correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = candy.values\n",
    "# inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5049808 ],\n",
       "       [0.60592761],\n",
       "       [0.45748719]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "np.random.seed(812)\n",
    "weights = np.random.random((3,1))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06341479],\n",
       "       [0.96246799],\n",
       "       [1.06341479],\n",
       "       ...,\n",
       "       [1.06341479],\n",
       "       [1.06341479],\n",
       "       [0.96246799]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum = np.dot(X, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74334258],\n",
       "       [0.72361567],\n",
       "       [0.74334258],\n",
       "       ...,\n",
       "       [0.74334258],\n",
       "       [0.74334258],\n",
       "       [0.72361567]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_outputs = sigmoid(weighted_sum)\n",
    "activated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74334258],\n",
       "       [0.72361567],\n",
       "       [0.74334258],\n",
       "       ...,\n",
       "       [0.74334258],\n",
       "       [0.74334258],\n",
       "       [0.72361567]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1+np.exp(-1*weighted_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_shaped = np.reshape(y, (10000,1))\n",
    "y_shaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_shaped[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustments[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74334258],\n",
       "       [0.72361567],\n",
       "       [0.74334258],\n",
       "       [0.6124179 ],\n",
       "       [0.82755477],\n",
       "       [0.74334258],\n",
       "       [0.6124179 ],\n",
       "       [0.82755477],\n",
       "       [0.6124179 ],\n",
       "       [0.6124179 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_outputs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_shaped[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25665742],\n",
       "       [ 0.27638433],\n",
       "       [ 0.25665742],\n",
       "       [-0.6124179 ],\n",
       "       [-0.82755477],\n",
       "       [ 0.25665742],\n",
       "       [-0.6124179 ],\n",
       "       [-0.82755477],\n",
       "       [-0.6124179 ],\n",
       "       [-0.6124179 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = y_shaped - activated_outputs\n",
    "error[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_df = pd.DataFrame(data=error)\n",
    "# error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05605741],\n",
       "       [ 0.06078562],\n",
       "       [ 0.05605741],\n",
       "       [-0.13960071],\n",
       "       [-0.17514999],\n",
       "       [ 0.05605741],\n",
       "       [-0.13960071],\n",
       "       [-0.17514999],\n",
       "       [-0.13960071],\n",
       "       [-0.13960071]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjustments = error * sigmoid_derivative(activated_outputs)\n",
    "adjustments[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjustments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-285.38531925],\n",
       "       [-296.96183722],\n",
       "       [-493.77958388]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights += np.dot(X.T, adjustments)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[4.78190965]\n",
      " [4.78380389]\n",
      " [8.60172108]]\n",
      "Output after training\n",
      "[[0.00624753]\n",
      " [0.00625124]\n",
      " [0.00624753]\n",
      " ...\n",
      " [0.00624753]\n",
      " [0.00624753]\n",
      " [0.00625124]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1000):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(X, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # CaLculation error\n",
    "    error = y_shaped - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)*.01\n",
    "    \n",
    "    # Update the Weights\n",
    "    weights += np.dot(X.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOT SURE WHY WEIGHTS ARE TRAINING TO THE SAME POLARITY (+/-)\n",
    "####  2 OF THE 3 SHOULD HAVE OPPOSITE POLARITY (+/-) OF THE 3RD WEIGHT\n",
    "####  IN ORDER TO PROPERLY PREDICT THE CHOCOLATE-GUMMY/NAND-GATE PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99375247],\n",
       "       [ 0.99374876],\n",
       "       [ 0.99375247],\n",
       "       [-0.02529353],\n",
       "       [-0.00152168],\n",
       "       [ 0.99375247],\n",
       "       [-0.02529353],\n",
       "       [-0.00152168],\n",
       "       [-0.02529353],\n",
       "       [-0.02529353],\n",
       "       [ 0.99375247],\n",
       "       [-0.00152168],\n",
       "       [-0.02529353],\n",
       "       [ 0.99375247],\n",
       "       [ 0.99375247],\n",
       "       [-0.00625124],\n",
       "       [ 0.99374876],\n",
       "       [ 0.99375247],\n",
       "       [-0.00152168],\n",
       "       [ 0.99375247]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00624753],\n",
       "       [0.00625124],\n",
       "       [0.00624753],\n",
       "       [0.02529353],\n",
       "       [0.00152168],\n",
       "       [0.00624753],\n",
       "       [0.02529353],\n",
       "       [0.00152168],\n",
       "       [0.02529353],\n",
       "       [0.02529353],\n",
       "       [0.00624753],\n",
       "       [0.00152168],\n",
       "       [0.02529353],\n",
       "       [0.00624753],\n",
       "       [0.00624753],\n",
       "       [0.00625124],\n",
       "       [0.00625124],\n",
       "       [0.00624753],\n",
       "       [0.00152168],\n",
       "       [0.00624753],\n",
       "       [0.00624753],\n",
       "       [0.00624753],\n",
       "       [0.00152168],\n",
       "       [0.00152168],\n",
       "       [0.00625124],\n",
       "       [0.00625124],\n",
       "       [0.02529353],\n",
       "       [0.02529353],\n",
       "       [0.02529353],\n",
       "       [0.02529353],\n",
       "       [0.00625124],\n",
       "       [0.00624753],\n",
       "       [0.00152168],\n",
       "       [0.00625124],\n",
       "       [0.00625124],\n",
       "       [0.00624753],\n",
       "       [0.00624753],\n",
       "       [0.00152168],\n",
       "       [0.02529353],\n",
       "       [0.00625124],\n",
       "       [0.00625124],\n",
       "       [0.02529353],\n",
       "       [0.00152168],\n",
       "       [0.00624753],\n",
       "       [0.00625124],\n",
       "       [0.00152168],\n",
       "       [0.02529353],\n",
       "       [0.00624753],\n",
       "       [0.02529353],\n",
       "       [0.02529353]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_output[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.48435693e-03],\n",
       "       [ 2.48434762e-03],\n",
       "       [ 2.48435693e-03],\n",
       "       [-6.32237187e-05],\n",
       "       [-3.80419493e-06],\n",
       "       [ 2.48435693e-03],\n",
       "       [-6.32237187e-05],\n",
       "       [-3.80419493e-06],\n",
       "       [-6.32237187e-05],\n",
       "       [-6.32237187e-05],\n",
       "       [ 2.48435693e-03],\n",
       "       [-3.80419493e-06],\n",
       "       [-6.32237187e-05],\n",
       "       [ 2.48435693e-03],\n",
       "       [ 2.48435693e-03],\n",
       "       [-1.56279548e-05],\n",
       "       [ 2.48434762e-03],\n",
       "       [ 2.48435693e-03],\n",
       "       [-3.80419493e-06],\n",
       "       [ 2.48435693e-03]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjustments[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00624753],\n",
       "       [0.00625124],\n",
       "       [0.00624753],\n",
       "       ...,\n",
       "       [0.00624753],\n",
       "       [0.00624753],\n",
       "       [0.00625124]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded = np.around(activated_output)\n",
    "rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = rounded\n",
    "y_true = y_shaped\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needs more nodes? Not certain on why the score was so low with a simple perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 3\n",
    "        self.hiddenNodes = 1\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 3x1 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 1x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum) \n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y_shaped,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y_shaped - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y_shaped):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y_shaped,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.54170017]\n",
      " [0.53821391]\n",
      " [0.54170017]\n",
      " ...\n",
      " [0.54170017]\n",
      " [0.54170017]\n",
      " [0.53821391]]\n",
      "Loss: \n",
      " 0.2513994616679112\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.20514292]\n",
      " [0.02190297]\n",
      " [0.20514292]\n",
      " ...\n",
      " [0.20514292]\n",
      " [0.20514292]\n",
      " [0.02190297]]\n",
      "Loss: \n",
      " 0.40957837555708776\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.25\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.25\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.25\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.25\n",
      "+---------EPOCH 2000---------+\n",
      "Input: \n",
      " [[0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.25\n",
      "+---------EPOCH 3000---------+\n",
      "Input: \n",
      " [[0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 0 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.25\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y_shaped)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y_shaped - nn.feed_forward(X)))))\n",
    "    nn.train(X,y_shaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAME ISSUE AS EARLIER; \n",
    "####  NOT SURE WHY WEIGHTS ARE TRAINING TO THE SAME POLARITY (+/-)\n",
    "####  2 OF THE 3 SHOULD HAVE OPPOSITE POLARITY (+/-) OF THE 3RD WEIGHT\n",
    "####  IN ORDER TO PROPERLY PREDICT THE CHOCOLATE-GUMMY/NAND-GATE PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bias'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
    "       'exang', 'oldpeak', 'slope', 'ca', 'thal','bias']].values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BELOW TUNING: NODES (14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model1 = Sequential(name=\"1\")\n",
    "\n",
    "model1.add(Dense(14, input_dim=14, activation='relu', name=\"Dense1\"))\n",
    "model1.add(Dense(14, activation='relu'))\n",
    "model1.add(Dense(14, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION SPLIT SAVES .N OF YOUR DATA (HERE=10%) FOR VALIDATION ACCURACY\n",
    "model1.fit(X,y, epochs=250, validation_split=.10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model1.evaluate(X,y, verbose=0)\n",
    "print(f\"{model1.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY OF 86.7986798286438 NODES (14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BELOW TUNING: NODES = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential(name=\"2\")\n",
    "\n",
    "model2.add(Dense(28, input_dim=14, activation='relu', name=\"Dense1\"))\n",
    "model2.add(Dense(28, activation='relu'))\n",
    "model2.add(Dense(28, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION SPLIT SAVES .N OF YOUR DATA (HERE=10%) FOR VALIDATION ACCURACY\n",
    "model2.fit(X,y, epochs=250, validation_split=.10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.evaluate(X,y, verbose=0)\n",
    "print(f\"{model2.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY OF 86.7986798286438 NODES (28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model3 = Sequential(name=\"3\")\n",
    "    model3.add(Dense(28, input_dim=14, activation='relu', name=\"Dense1\"))\n",
    "    model3.add(Dense(28, activation='relu'))\n",
    "    model3.add(Dense(28, activation='relu'))\n",
    "    model3.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(842)\n",
    "model3 = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'batch_size': [10,15, 20,30, 40, 60, 80, 100],\n",
    "              'epochs': [20,40,60]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model3, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  GRID SEARCH RESULTS\n",
    "####  BEST BATCH SIZE = 15 (VIA GRID SEARCH)\n",
    "####  BEST EPOCH SIZE = 60 (HIGHER = BETTER)(VIA GRID SEARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential(name=\"4\")\n",
    "\n",
    "model4.add(Dense(28, input_dim=14, activation='relu', name=\"Dense1\"))\n",
    "model4.add(Dense(28, activation='relu'))\n",
    "model4.add(Dense(28, activation='relu'))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION SPLIT SAVES .N OF YOUR DATA (HERE=10%) FOR VALIDATION ACCURACY\n",
    "model4.fit(X,y, batch_size=80, epochs=1500, validation_split=.10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model4.evaluate(X,y, verbose=0)\n",
    "print(f\"{model4.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  HYPERPARAMETERS TUNED: BATCH (80), EPOCH SIZE (1500), AND NODES (28)\n",
    "####  BEST ACCURACY SCORE: GIVES ALTERNATING VALUES OF 45.54455578327179 OR 87.78877854347229...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential(name=\"5\")\n",
    "\n",
    "model5.add(Dense(14, input_dim=14, activation='relu', name=\"Dense1\"))\n",
    "model5.add(Dense(14, activation='relu'))\n",
    "model5.add(Dense(14, activation='relu'))\n",
    "model5.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model5.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION SPLIT SAVES .N OF YOUR DATA (HERE=10%) FOR VALIDATION ACCURACY\n",
    "model5.fit(X,y, batch_size=15, epochs=1500, validation_split=.10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model5.evaluate(X,y, verbose=0)\n",
    "print(f\"{model5.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  HYPERPARAMETERS TUNED: BATCH (15), EPOCH SIZE (1500) AND NODES (14)\n",
    "####  BEST ACCURACY SCORE: 88.11880946159363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential(name=\"6\")\n",
    "\n",
    "model6.add(Dense(14, input_dim=14, activation='relu', name=\"Dense1\"))\n",
    "model6.add(Dense(14, activation='relu'))\n",
    "model6.add(Dense(28, activation='relu'))\n",
    "model6.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model6.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION SPLIT SAVES .N OF YOUR DATA (HERE=10%) FOR VALIDATION ACCURACY\n",
    "model7.fit(X,y, batch_size=15, epochs=1500, validation_split=.10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model7.evaluate(X,y, verbose=0)\n",
    "print(f\"{model7.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS TUNED: BATCH (15), EPOCH SIZE(1500), NODES(14/28)\n",
    "# BEST ACCURACY SCORE: 99.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Sequential(name=\"7\")\n",
    "\n",
    "model7.add(Dense(14, input_dim=14, activation='relu', name=\"Dense1\"))\n",
    "model7.add(Dense(14, activation='relu'))\n",
    "model7.add(Dense(28, activation='relu'))\n",
    "model7.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model7.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Let's inspect our new architecture\n",
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION SPLIT SAVES .N OF YOUR DATA (HERE=10%) FOR VALIDATION ACCURACY\n",
    "model7.fit(X,y, batch_size=15, epochs=40, validation_split=.10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model7.evaluate(X,y, verbose=0)\n",
    "print(f\"{model7.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "U4-S1-M1 (Python3)",
   "language": "python",
   "name": "u4-s1-m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
